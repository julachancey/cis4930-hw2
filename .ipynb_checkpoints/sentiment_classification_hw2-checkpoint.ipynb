{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32bea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2ec3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7ff97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1048575\n",
      "Number of rows: 359\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the dataset\n",
    "print('Number of rows:', len(df))\n",
    "print('Number of rows:', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce1e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    800000\n",
      "1    248575\n",
      "Name: Sentiment, dtype: int64\n",
      "1    182\n",
      "0    177\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the sentiment distribution of the dataset\n",
    "print(df['Sentiment'].value_counts())\n",
    "print(test_df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3ac3b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index        0\n",
      "Sentiment    0\n",
      "Text         0\n",
      "dtype: int64\n",
      "Index        0\n",
      "Sentiment    0\n",
      "Text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any missing values\n",
    "print(df.isnull().sum())\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb17acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/julia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "340c43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stop words from the text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.casefold() not in stop_words]\n",
    "\n",
    "    # Join the filtered words back into a single string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "# Apply the preprocessing function to the text column\n",
    "df['text'] = df['Text'].apply(clean_text)\n",
    "test_df['text'] = test_df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f8f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201b8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words\n",
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c67a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf = tfidf_vect.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcc288be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "import numpy as np\n",
    "sentences = [text.split() for text in df['text']]\n",
    "word2vec_model = Word2Vec(sentences, window=5, min_count=1)\n",
    "word2vec_X = []\n",
    "for sentence in sentences:\n",
    "    sentence_vec = np.zeros((100,))\n",
    "    for word in sentence:\n",
    "        if word in word2vec_model.wv:\n",
    "            sentence_vec += word2vec_model.wv[word]\n",
    "    word2vec_X.append(sentence_vec)\n",
    "word2vec_X = np.array(word2vec_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b566a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Classification Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcbfac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index  Sentiment                                               Text  \\\n",
      "0      0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1      1          0  is upset that he can't update his Facebook by ...   \n",
      "2      2          0  @Kenichan I dived many times for the ball. Man...   \n",
      "3      3          0    my whole body feels itchy and like its on fire    \n",
      "\n",
      "                                                text  \n",
      "0  switchfoot httptwitpiccomyzl awww thats bummer...  \n",
      "1  upset cant update facebook texting might cry r...  \n",
      "2  kenichan dived many times ball managed save re...  \n",
      "3                   whole body feels itchy like fire  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e2e56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow, df['Sentiment'], test_size=0.2, random_state=42)\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf, df['Sentiment'], test_size=0.2, random_state=42)\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(word2vec_X, df['Sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2938a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr_bow = LogisticRegression(max_iter=1000000)\n",
    "lr_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "lr_tfidf = LogisticRegression(max_iter=1000000)\n",
    "lr_tfidf.fit(X_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier\n",
    "svm_bow = SVC()\n",
    "svm_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "svm_tfidf = SVC()\n",
    "svm_tfidf.fit(X_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier\n",
    "nbc_bow = MultinomialNB()\n",
    "nbc_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "nbc_tfidf = MultinomialNB()\n",
    "nbc_tfidf.fit(X_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44871714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest classifier\n",
    "rfc_bow = RandomForestClassifier()\n",
    "rfc_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "rfc_tfidf = RandomForestClassifier()\n",
    "rfc_tfidf.fit(X_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68422921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# create MLP model\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(128, activation='relu', input_dim=X_train_w2v.shape[1]))\n",
    "mlp_model.add(Dropout(0.2))\n",
    "mlp_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile model\n",
    "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "mlp_model.fit(X_train_word2vec, y_train_word2vec, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# LSTM model\n",
    "from keras.layers import LSTM, Embedding\n",
    "\n",
    "# create LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length))\n",
    "lstm_model.add(LSTM(units=64))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile model\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "lstm_model.fit(X_train_word2vec, y_train_word2vec, epochs=10, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706aa4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_lr_bow = lr_bow.predict(X_test_bow)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print('Logistic Regression Bag of Words')\n",
    "print('Accuracy:', accuracy_score(y_test_bow, y_pred_lr_bow))\n",
    "print('Precision:', precision_score(y_test_bow, y_pred_lr_bow, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_bow, y_pred_lr_bow, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_bow, y_pred_lr_bow, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ce908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred_lr_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print('Logistic Regression TF-IDF')\n",
    "print('Accuracy:', accuracy_score(y_test_tfidf, y_pred_lr_tfidf))\n",
    "print('Precision:', precision_score(y_test_tfidf, y_pred_lr_tfidf, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_tfidf, y_pred_lr_tfidf, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_tfidf, y_pred_lr_tfidf, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd484c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred_svm_bow = svm_bow.predict(X_test_bow)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print('SVM classifier Bag of Words')\n",
    "print('Accuracy:', accuracy_score(y_test_bow, y_pred_svm_bow))\n",
    "print('Precision:', precision_score(y_test_bow, y_pred_svm_bow, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_bow, y_pred_svm_bow, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_bow, y_pred_svm_bow, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e533627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred_svm_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print('SVM classifier TF-IDF')\n",
    "print('Accuracy:', accuracy_score(y_test_tfidf, y_pred_svm_tfidf))\n",
    "print('Precision:', precision_score(y_test_tfidf, y_pred_svm_tfidf, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_tfidf, y_pred_svm_tfidf, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_tfidf, y_pred_svm_tfidf, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0167e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred_nbc_bow = nbc_bow.predict(X_test_bow)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print('Naive Bayes Bag of Words')\n",
    "print('Accuracy:', accuracy_score(y_test_bow, y_pred_nbc_bow))\n",
    "print('Precision:', precision_score(y_test_bow, y_pred_nbc_bow, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_bow, y_pred_nbc_bow, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_bow, y_pred_nbc_bow, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred_nbc_tfidf = nbc_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print('Naive Bayes TF-IDF')\n",
    "print('Accuracy:', accuracy_score(y_test_tfidf, y_pred_nbc_tfidf))\n",
    "print('Precision:', precision_score(y_test_tfidf, y_pred_nbc_tfidf, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_tfidf, y_pred_nbc_tfidf, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_tfidf, y_pred_nbc_tfidf, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred_rfc_bow = rfc_bow.predict(X_test_bow)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print('Random Forest Bag of Words')\n",
    "print('Accuracy:', accuracy_score(y_test_bow, y_pred_rfc_bow))\n",
    "print('Precision:', precision_score(y_test_bow, y_pred_rfc_bow, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_bow, y_pred_rfc_bow, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_bow, y_pred_rfc_bow, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred_rfc_tfidf = rfc_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print('Random Forest TF-IDF')\n",
    "print('Accuracy:', accuracy_score(y_test_tfidf, y_pred_rfc_tfidf))\n",
    "print('Precision:', precision_score(y_test_tfidf, y_pred_rfc_tfidf, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_tfidf, y_pred_rfc_tfidf, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_tfidf, y_pred_rfc_tfidf, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred_mlp = mlp_model.predict(X_test_word2vec)\n",
    "y_pred_lstm = lstm_model.predict_classes(X_test_word2vec)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print('MLP - multilayer perceptron - Word2Vec')\n",
    "print('Accuracy:', accuracy_score(y_test_word2vec, y_pred_mlp))\n",
    "print('Precision:', precision_score(y_test_word2vec, y_pred_mlp, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_word2vec, y_pred_mlp, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_word2vec, y_pred_mlp, average='weighted'))\n",
    "\n",
    "print('LSTM - long short-term memory - Word2Vec')\n",
    "print('Accuracy:', accuracy_score(y_test_word2vec, y_pred_lstm))\n",
    "print('Precision:', precision_score(y_test_word2vec, y_pred_lstm, average='weighted'))\n",
    "print('Recall:', recall_score(y_test_word2vec, y_pred_lstm, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test_word2vec, y_pred_lstm, average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
